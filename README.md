LLM prompt injection test framework for testing attacks against safety models such as Llama Guard. 
